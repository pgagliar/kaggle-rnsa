{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import glob\n",
    "import pydicom\n",
    "ROOT_DIR = '/home/gagliardi/rnsa'\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'SSD-Tensorflow'))  # To find local version of the library\n",
    "from datasets import dataset_factory, dataset_utils\n",
    "from nets import ssd_vgg_300, ssd_vgg_512, ssd_common, np_methods\n",
    "from preprocessing import ssd_vgg_preprocessing\n",
    "from datasets import pascalvoc_to_tfrecords\n",
    "import tf_utils\n",
    "import ntpath\n",
    "import numpy as np\n",
    "import parse_tfrecord\n",
    "\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TfRecordData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob.glob('./data/tf_record_data/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building TF graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================== #\n",
    "# Flag reinitialisation and add a f flag for jupyter kernel\n",
    "# =========================================================================== #\n",
    "from absl import flags\n",
    "for name in list(flags.FLAGS):\n",
    "    delattr(flags.FLAGS, name)\n",
    "    \n",
    "# =========================================================================== #\n",
    "# SSD Network flags.\n",
    "# =========================================================================== #\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'loss_alpha', 1., 'Alpha parameter in the loss function.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'negative_ratio', 3., 'Negative ratio in the loss function.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'match_threshold', 0.5, 'Matching threshold in the loss function.')\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')    \n",
    "# =========================================================================== #\n",
    "# Learning Rate Flags.\n",
    "# =========================================================================== #\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'learning_rate_decay_type',\n",
    "    'exponential',\n",
    "    'Specifies how the learning rate is decayed. One of \"fixed\", \"exponential\",'\n",
    "    ' or \"polynomial\"')\n",
    "tf.app.flags.DEFINE_float('learning_rate', 0.001, 'Initial learning rate.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'end_learning_rate', 0.0001,\n",
    "    'The minimal end learning rate used by a polynomial decay learning rate.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'label_smoothing', 0.0, 'The amount of label smoothing.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'learning_rate_decay_factor', 0.94, 'Learning rate decay factor.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'num_epochs_per_decay', 2.0,\n",
    "    'Number of epochs after which learning rate decays.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'moving_average_decay', None,\n",
    "    'The decay to use for the moving average.'\n",
    "    'If left as None, then moving averages are not used.')\n",
    "# =========================================================================== #\n",
    "# Dataset Flags.\n",
    "# =========================================================================== #\n",
    "tf.app.flags.DEFINE_integer('batch_size', 16, 'The number of samples in each batch.')\n",
    "tf.app.flags.DEFINE_integer( 'num_readers', 4, 'The number of parallel readers that read data from the dataset.')\n",
    "\n",
    "# =========================================================================== #\n",
    "# Optimization Flags.\n",
    "# =========================================================================== #\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'weight_decay', 0.00004, 'The weight decay on the model weights.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'optimizer', 'rmsprop',\n",
    "    'The name of the optimizer, one of \"adadelta\", \"adagrad\", \"adam\",'\n",
    "    '\"ftrl\", \"momentum\", \"sgd\" or \"rmsprop\".')\n",
    "tf.app.flags.DEFINE_float('rmsprop_momentum', 0.9, 'Momentum.')\n",
    "tf.app.flags.DEFINE_float('rmsprop_decay', 0.9, 'Decay term for RMSProp.')\n",
    "tf.app.flags.DEFINE_float('opt_epsilon', 1.0, 'Epsilon term for the optimizer.')\n",
    "\n",
    "# =========================================================================== #\n",
    "# Summary Flags.\n",
    "# =========================================================================== #\n",
    "tf.app.flags.DEFINE_string('summaries_dir','./logdir/','Summary')\n",
    "\n",
    "# =========================================================================== #\n",
    "# Fine-Tuning Flags.\n",
    "# =========================================================================== #\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'checkpoint_path', None,\n",
    "    'The path to a checkpoint from which to fine-tune.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'checkpoint_model_scope', None,\n",
    "    'Model scope in the checkpoint. None if the same as the trained model.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'checkpoint_exclude_scopes', None,\n",
    "    'Comma-separated list of scopes of variables to exclude when restoring '\n",
    "    'from a checkpoint.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'trainable_scopes', None,\n",
    "    'Comma-separated list of scopes to filter the set of variables to train.'\n",
    "    'By default, None would train all the variables.')\n",
    "tf.app.flags.DEFINE_boolean(\n",
    "    'ignore_missing_vars', False,\n",
    "    'When restoring a checkpoint would ignore missing variables.')\n",
    "\n",
    "# =========================================================================== #\n",
    "# epochs Flags.\n",
    "# =========================================================================== #\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'epochs', 10,\n",
    "    'The number of epochs.')\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSD - preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_preprocessing_threads = 8\n",
    "\n",
    "#image = tf.placeholder(tf.uint8, shape=(1024, 1024, 3))\n",
    "#labels = tf.placeholder(tf.int64, shape=(1,))\n",
    "#bboxes = tf.placeholder(tf.float32, shape=(None, 4))\n",
    "\n",
    "ssd_net = ssd_vgg_512.SSDNet() \n",
    "\n",
    "ssd_shape = ssd_net.params.img_shape\n",
    "ssd_anchors = ssd_net.anchors(ssd_shape)\n",
    "    \n",
    "# resizing pictures\n",
    "out_shape = tf.constant([512, 512])\n",
    "#Num_samples x Height x Width x Channels\n",
    "DATA_FORMAT = 'NHWC'\n",
    "iterator = parse_tfrecord.get_iterator(filenames)\n",
    "filename, image, labels, bboxes = iterator.get_next()\n",
    "\n",
    "# Pre-processing image, labels and bboxes.\n",
    "image, glabels, gbboxes = ssd_vgg_preprocessing.preprocess_image(image, labels, bboxes, out_shape, data_format=DATA_FORMAT, is_training=True)\n",
    "\n",
    "# Encode groundtruth labels and bboxes.\n",
    "gclasses, glocalisations, gscores = ssd_net.bboxes_encode(glabels, gbboxes, ssd_anchors)\n",
    "batch_shape = [1] + [len(ssd_anchors)] * 3\n",
    "\n",
    "# Training batches and queue.\n",
    "r = tf.train.batch( tf_utils.reshape_list([image, gclasses, glocalisations, gscores]), \n",
    "                    batch_size=FLAGS.batch_size,\n",
    "                    num_threads=num_preprocessing_threads,\n",
    "                    capacity=5 * FLAGS.batch_size)\n",
    "\n",
    "b_image, b_gclasses, b_glocalisations, b_gscores = tf_utils.reshape_list(r, batch_shape)\n",
    "\n",
    "# Intermediate queueing: unique batch computation pipeline for all\n",
    "# GPUs running the training.\n",
    "batch_queue = slim.prefetch_queue.prefetch_queue(tf_utils.reshape_list([b_image, b_gclasses, b_glocalisations, b_gscores]), capacity=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Definte the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================== #\n",
    "# Define the model running on every GPU.\n",
    "# =================================================================== #\n",
    "    \n",
    "b_image, b_gclasses, b_glocalisations, b_gscores = tf_utils.reshape_list(batch_queue.dequeue(), batch_shape)\n",
    "\n",
    "# Construct SSD network.\n",
    "arg_scope = ssd_net.arg_scope(weight_decay=FLAGS.weight_decay, data_format=DATA_FORMAT)\n",
    "\n",
    "with slim.arg_scope(arg_scope):\n",
    "    predictions, localisations, logits, end_points = ssd_net.net(b_image, is_training=True)\n",
    "\n",
    "\n",
    "# Add loss function.\n",
    "ssd_net.losses(logits, \n",
    "               localisations,\n",
    "               b_gclasses, \n",
    "               b_glocalisations, \n",
    "               b_gscores,\n",
    "               match_threshold=FLAGS.match_threshold,\n",
    "               negative_ratio=FLAGS.negative_ratio,\n",
    "               alpha=FLAGS.loss_alpha,\n",
    "               label_smoothing=FLAGS.label_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSD - set optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 5659\n",
    "\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n",
    "\n",
    "learning_rate = tf_utils.configure_learning_rate(FLAGS, num_samples, global_step)\n",
    "optimizer = tf_utils.configure_optimizer(FLAGS, learning_rate)\n",
    "\n",
    "summaries.add(tf.summary.scalar('global_step', global_step))\n",
    "summaries.add(tf.summary.scalar('learning_rate', learning_rate))\n",
    "\n",
    "total_loss = tf.add_n(tf.get_collection(tf.GraphKeys.LOSSES))\n",
    "\n",
    "\n",
    "train = optimizer.minimize(total_loss)\n",
    "\n",
    "for loss in tf.get_collection(tf.GraphKeys.LOSSES):\n",
    "    summaries.add(tf.summary.scalar(loss.op.name, loss))\n",
    "\n",
    "# Add summaries for variables.\n",
    "for variable in slim.get_model_variables():\n",
    "    summaries.add(tf.summary.histogram(variable.op.name, variable))\n",
    "\n",
    "summaries.add(tf.summary.scalar('total_loss', total_loss))\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "#sess.run(iterator.initializer)    \n",
    "# Run training.\n",
    "#slim.learning.train(train, logdir='./logdir', init_fn=init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(iterator.initializer)\n",
    "\n",
    "accuracy_list = []\n",
    "\n",
    "while not sess.should_stop():\n",
    "    summaries, _, acc = sess.run([merged, train, total_loss])\n",
    "    accuracy_list.append(acc)\n",
    "    if i%100==0:\n",
    "        print('Accuracy at step %s: %s' % (i, sum(accuracy_list)/len(accuracy_list)))\n",
    "        accuracy_list = []\n",
    "    train_writer.add_summary(summaries, i)\n",
    "'''\n",
    "EPOCHS = 45\n",
    "#df = pd.read_csv('./data/stage_1_train_labels.csv')\n",
    "#df[df.Target==1].patientId.nunique()\n",
    "n_batches = int(np.floor(5659/FLAGS.batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    train_writer = tf.summary.FileWriter(FLAGS.summaries_dir, sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(iterator.initializer)\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    print('Training...')\n",
    "    \n",
    "    for i in range(EPOCHS):\n",
    "        tot_loss = 0\n",
    "        for j in range(n_batches):\n",
    "            summaries, _, loss_value = sess.run([merged, train, total_loss])\n",
    "            \n",
    "            tot_loss += loss_value\n",
    "            train_writer.add_summary(summaries, j)\n",
    "        print(\"Iter: {}, Loss: {:.4f}\".format(i, tot_loss / n_batches))\n",
    "    saver.save(sess, FLAGS.summaries_dir+'ssd_512.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "std_conda_3",
   "language": "python",
   "name": "std_conda_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
